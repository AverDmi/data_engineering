{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955a3618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/02/27 10:58:51 WARN Utils: Your hostname, thomas-comp resolves to a loopback address: 127.0.1.1; using 192.168.0.160 instead (on interface enp34s0)\n",
      "22/02/27 10:58:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/thomas/spark_trening/env/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.2.1\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.13\n",
      "Branch HEAD\n",
      "Compiled by user hgao on 2022-01-20T19:26:14Z\n",
      "Revision 4f25b3f71238a00508a356591553f2dfa89f8290\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dcdbf7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62319ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"test\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc4bb174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-27 11:11:35--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv\n",
      "Распознаётся nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)... 52.217.87.100\n",
      "Подключение к nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)|52.217.87.100|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа... 200 OK\n",
      "Длина: 733822658 (700M) [text/csv]\n",
      "Сохранение в каталог: ««fhvhv_tripdata_2021-02.csv.1»».\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 699,83M  1,80MB/s    за 4m 11s  \n",
      "\n",
      "2022-02-27 11:15:47 (2,79 MB/s) - «fhvhv_tripdata_2021-02.csv.1» сохранён [733822658/733822658]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86a577fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11613943 fhvhv_tripdata_2021-02.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21ab7d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3951cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b098caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('head.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43d4d62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,StringType,true),StructField(dropoff_datetime,StringType,true),StructField(PULocationID,LongType,true),StructField(DOLocationID,LongType,true),StructField(SR_Flag,DoubleType,true)))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "641e5ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d29e3bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1bec3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a53aba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e225a830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/27 13:42:57 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "22/02/27 13:42:57 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/02/27 13:42:57 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/02/27 13:42:57 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/02/27 13:42:57 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "22/02/27 13:42:58 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/02/27 13:42:58 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/02/27 13:42:58 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/02/27 13:42:58 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/02/27 13:42:58 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/02/27 13:42:58 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/02/27 13:42:58 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/02/27 13:42:58 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "22/02/27 13:42:59 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/02/27 13:42:59 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/02/27 13:42:59 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/02/27 13:42:59 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "22/02/27 13:42:59 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/02/27 13:42:59 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "22/02/27 13:42:59 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/02/27 13:42:59 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "22/02/27 13:42:59 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/02/27 13:42:59 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "22/02/27 13:42:59 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/02/27 13:42:59 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/02/27 13:42:59 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/02/27 13:42:59 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "22/02/27 13:42:59 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/02/27 13:42:59 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "22/02/27 13:43:00 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/02/27 13:43:00 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/02/27 13:43:00 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/02/27 13:43:00 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhvhv/2021/01/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "532616c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bae47638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:==========================================>              (9 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-02-01|  2021-02-01|         106|         106|\n",
      "| 2021-02-02|  2021-02-02|          37|          25|\n",
      "| 2021-02-01|  2021-02-01|          62|          61|\n",
      "| 2021-02-03|  2021-02-03|          18|         147|\n",
      "| 2021-02-01|  2021-02-01|          74|          74|\n",
      "| 2021-02-01|  2021-02-01|          76|         117|\n",
      "| 2021-02-01|  2021-02-01|         191|          98|\n",
      "| 2021-02-03|  2021-02-03|          48|         265|\n",
      "| 2021-02-03|  2021-02-03|          17|         217|\n",
      "| 2021-02-03|  2021-02-03|         236|         170|\n",
      "| 2021-02-03|  2021-02-04|         162|         145|\n",
      "| 2021-02-02|  2021-02-02|         243|         162|\n",
      "| 2021-02-03|  2021-02-03|         138|          75|\n",
      "| 2021-02-04|  2021-02-04|          67|         228|\n",
      "| 2021-02-02|  2021-02-02|         130|         180|\n",
      "| 2021-02-02|  2021-02-02|          47|          51|\n",
      "| 2021-02-03|  2021-02-03|          61|         177|\n",
      "| 2021-02-01|  2021-02-01|         215|         122|\n",
      "| 2021-02-03|  2021-02-03|         213|          32|\n",
      "| 2021-02-01|  2021-02-01|         231|         163|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6315c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "40d4acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn('long_of_trip', (F.col(\"dropoff_datetime\").cast(\"long\") - F.col(\"pickup_datetime\").cast(\"long\"))/60) \\\n",
    "    .select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID', 'long_of_trip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b4b3276b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:======================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+------------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|      long_of_trip|\n",
      "+-----------+------------+------------+------------+------------------+\n",
      "| 2021-02-11|  2021-02-12|         247|          41|            1259.0|\n",
      "| 2021-02-17|  2021-02-18|         242|         254| 953.6833333333333|\n",
      "| 2021-02-20|  2021-02-21|         188|          55| 733.9833333333333|\n",
      "| 2021-02-03|  2021-02-04|          51|         147|            677.55|\n",
      "| 2021-02-19|  2021-02-20|         210|         149| 626.2833333333333|\n",
      "| 2021-02-25|  2021-02-26|         174|         126|             583.5|\n",
      "| 2021-02-20|  2021-02-20|         242|          31|             580.1|\n",
      "| 2021-02-18|  2021-02-19|         196|         197| 576.8666666666667|\n",
      "| 2021-02-18|  2021-02-18|          89|         265| 575.9166666666666|\n",
      "| 2021-02-10|  2021-02-11|         254|         259| 569.4833333333333|\n",
      "| 2021-02-10|  2021-02-10|          61|         265| 541.2666666666667|\n",
      "| 2021-02-25|  2021-02-25|         169|         265|            540.65|\n",
      "| 2021-02-21|  2021-02-22|          10|          10|            537.05|\n",
      "| 2021-02-09|  2021-02-10|          78|         147| 534.7833333333333|\n",
      "| 2021-02-06|  2021-02-06|         229|         188| 524.1166666666667|\n",
      "| 2021-02-02|  2021-02-02|          85|          85| 515.2166666666667|\n",
      "| 2021-02-10|  2021-02-10|          29|         125| 514.2666666666667|\n",
      "| 2021-02-09|  2021-02-09|         188|         265|             512.2|\n",
      "| 2021-02-21|  2021-02-22|         177|          73|             511.0|\n",
      "| 2021-02-05|  2021-02-06|          97|          72|508.51666666666665|\n",
      "+-----------+------------+------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 58:===========================>                           (12 + 12) / 24]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_df.orderBy(F.col(\"long_of_trip\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "306fe5d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:=================================>                       (7 + 5) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02867|2021-02-02 22:50:03|2021-02-02 23:12:18|         134|         179|   null|\n",
      "|           HV0003|              B02872|2021-02-03 07:44:08|2021-02-03 08:00:06|          48|         140|   null|\n",
      "|           HV0003|              B02871|2021-02-02 18:45:47|2021-02-02 19:04:51|          39|          35|   null|\n",
      "|           HV0003|              B02871|2021-02-01 19:36:44|2021-02-01 19:45:31|         226|         146|   null|\n",
      "|           HV0005|              B02510|2021-02-02 12:25:25|2021-02-02 12:46:33|         130|         131|   null|\n",
      "|           HV0003|              B02878|2021-02-01 14:54:37|2021-02-01 15:38:30|         180|         217|   null|\n",
      "|           HV0005|              B02510|2021-02-03 09:16:22|2021-02-03 09:21:58|         197|         197|   null|\n",
      "|           HV0003|              B02682|2021-02-03 09:47:59|2021-02-03 10:28:58|         251|         234|   null|\n",
      "|           HV0003|              B02617|2021-02-02 18:52:52|2021-02-02 19:03:51|          89|          91|   null|\n",
      "|           HV0003|              B02889|2021-02-01 00:12:32|2021-02-01 00:49:28|          50|         119|   null|\n",
      "|           HV0003|              B02764|2021-02-01 14:04:39|2021-02-01 14:13:34|         117|         117|   null|\n",
      "|           HV0003|              B02765|2021-02-03 04:30:30|2021-02-03 05:05:25|         177|          34|   null|\n",
      "|           HV0005|              B02510|2021-02-02 15:43:22|2021-02-02 16:00:27|         171|         196|   null|\n",
      "|           HV0003|              B02871|2021-02-03 13:44:04|2021-02-03 13:52:51|         210|          55|   null|\n",
      "|           HV0005|              B02510|2021-02-03 09:41:53|2021-02-03 10:01:44|          69|         213|   null|\n",
      "|           HV0003|              B02876|2021-02-03 15:46:40|2021-02-03 16:32:28|          36|         188|   null|\n",
      "|           HV0003|              B02870|2021-02-03 09:13:45|2021-02-03 09:19:35|         127|         243|   null|\n",
      "|           HV0003|              B02871|2021-02-03 18:49:22|2021-02-03 18:54:36|         210|          55|   null|\n",
      "|           HV0003|              B02682|2021-02-03 16:34:53|2021-02-03 17:08:34|         230|          74|   null|\n",
      "|           HV0003|              B02395|2021-02-03 10:13:28|2021-02-03 10:22:08|         126|         126|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8f6ace74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/spark_trening/env/lib/python3.8/site-packages/pyspark/sql/dataframe.py:138: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('df_feb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8612ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    dispatching_base_num,\n",
    "    COUNT(1) AS num_of_trips\n",
    "FROM df_feb\n",
    "GROUP BY dispatching_base_num\n",
    "ORDER BY num_of_trips DESC\n",
    "LIMIT 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e032da45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:===================>                                     (4 + 8) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|dispatching_base_num|num_of_trips|\n",
      "+--------------------+------------+\n",
      "|              B02510|     3233664|\n",
      "+--------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "58690492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 04_pyspark.ipynb\t      head.csv\r\n",
      " env\t\t\t      spark-warehouse\r\n",
      " fhvhv\t\t\t      taxi+_zone_lookup.csv\r\n",
      " fhvhv_tripdata_2021-01.csv   zones\r\n",
      " fhvhv_tripdata_2021-02.csv  'Без названия.ipynb'\r\n",
      " first_connect.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d9baa5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bbf21f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "85eedfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "puzones = df.join(df_zones, (df.PULocationID == df_zones.LocationID), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "41584645",
   "metadata": {},
   "outputs": [],
   "source": [
    "dozones = df.join(df_zones, (df.DOLocationID == df_zones.LocationID), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b35adb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/spark_trening/env/lib/python3.8/site-packages/pyspark/sql/dataframe.py:138: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "puzones.registerTempTable('puzones_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8d5a8ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_2 = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    Zone,\n",
    "    COUNT(1) AS number_of_count\n",
    "FROM puzones_table\n",
    "GROUP BY Zone\n",
    "ORDER BY number_of_count DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "085b6885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|                Zone|number_of_count|\n",
      "+--------------------+---------------+\n",
      "| Crown Heights North|         203777|\n",
      "|       East New York|         166959|\n",
      "|      Bushwick South|         140636|\n",
      "|        East Village|         137901|\n",
      "|Central Harlem North|         137246|\n",
      "|             Bedford|         125394|\n",
      "|  Stuyvesant Heights|         120026|\n",
      "|            Canarsie|         117751|\n",
      "|Prospect-Lefferts...|         117734|\n",
      "|Flatbush/Ditmas Park|         117092|\n",
      "|Washington Height...|         115590|\n",
      "|             Astoria|         110787|\n",
      "|TriBeCa/Civic Center|         109074|\n",
      "|   East Harlem North|         103440|\n",
      "|          Park Slope|         103267|\n",
      "|         Brownsville|         103059|\n",
      "|            Union Sq|          96469|\n",
      "|         Murray Hill|          94307|\n",
      "|        West Village|          93355|\n",
      "|   East Harlem South|          91517|\n",
      "+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b44d4c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dozones.registerTempTable('dozones_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "598fbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_2 = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    Zone,\n",
    "    COUNT(1) AS number_of_count\n",
    "FROM dozones_table\n",
    "GROUP BY Zone\n",
    "ORDER BY number_of_count DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a89018d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|                Zone|number_of_count|\n",
      "+--------------------+---------------+\n",
      "|                  NA|         357369|\n",
      "| Crown Heights North|         202250|\n",
      "|       East New York|         168094|\n",
      "|      Bushwick South|         140826|\n",
      "|Central Harlem North|         127432|\n",
      "|             Bedford|         121202|\n",
      "|        East Village|         120423|\n",
      "|  Stuyvesant Heights|         118566|\n",
      "|Prospect-Lefferts...|         117480|\n",
      "|Washington Height...|         115897|\n",
      "|Flatbush/Ditmas Park|         115025|\n",
      "|            Canarsie|         112124|\n",
      "|             Astoria|         105745|\n",
      "|         JFK Airport|         104386|\n",
      "|          Park Slope|         102323|\n",
      "|TriBeCa/Civic Center|          98220|\n",
      "|   East Harlem North|          97698|\n",
      "|         Brownsville|          97433|\n",
      "|            Union Sq|          95025|\n",
      "|         Murray Hill|          94618|\n",
      "+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5107faa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
